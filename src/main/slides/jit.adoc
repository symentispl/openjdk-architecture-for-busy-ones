== time for just-in-time compiler

* compilation policy
* profile guided optimizations with method counters & method data
* compile broker, queues and tasks
* speculating compiler & deoptimizations & uncommon traps

=== a few words about JIT

JIT (just-in-time compiler) in JVM was a major step in the world of JITs

* profile guided optimizations
* speculating compilation (with traps and deoptimizations)
* actually two not one compiler
* on-stack replacement
* used SSA (single static assigment) form (using "sea of nodes" developed by Cliff Click)

=== !

we are not going to dive into implemented optimizations, +
register allocations, +
escape analisys, +
inlining

will focus on JIT infrastructure code

[role="highlight_section_title"]
=== need for speed

image::https://media.giphy.com/media/xTiTnFM0Cr2xcGUsVy/giphy.gif[background]


=== !

so, who decides when code gets compiled? +
(and deoptimized)

`src/hotspot/share/compiler/compilationPolicy.hpp`

let's disassemble the first parts

=== !

The system supports 5 execution levels:

* level 0 - interpreter
* level 1 - C1 with full optimization (no profiling)
* level 2 - C1 with invocation and backedge counters
* level 3 - C1 with full profiling (level 2 + MDO)
* level 4 - C2

=== !

Levels 0, 2 and 3 periodically notify the runtime about the current value of the counters (invocation counters and backedge counters). The frequency of these notifications is different at each level. These notifications are used by the policy to decide what transition to make.

=== backedge counters

invocation counter are pretty obvious 

what the hell is backedge counter?

[source,java]
----
for(;;){

} // increment backedge counter
----

=== !

Execution starts at level 0 (interpreter), then the policy can decide either to compile the method at level 3 or level 2. The decision is based on the following factors:

=== !

The length of the C2 queue determines the next level. The observation is that level 2 is generally faster than level 3 by about 30%, therefore we would want to minimize the time a method spends at level 3. We should only spend the time at level 3 that is necessary to get adequate profiling. So, if the C2 queue is long enough it is more beneficial to go first to level 2, because if we transitioned to level 3 we would be stuck there until our C2 compile request makes its way through the long queue. When the load on C2 recedes we are going to recompile at level 3 and start gathering profiling information.

=== queue? not again

there are few components shared by both compilers, +
`src/hotspot/share/compiler/compileBroker.hpp`:

* CompileQueue
* CompileTask
* CompileBroker

=== !

when compilation policy decides that method should be compiled, it puts a method (a compile task) onto one of the queues

by default C1 queue has one worker threads and C2 has two (it all depends on your machine)

=== !

The length of C1 queue is used to dynamically adjust the thresholds, so as to introduce additional filtering if the compiler is overloaded. The rationale is that by the time a method gets compiled it can become unused, so it doesn't make sense to put too much onto the queue.
 
=== !

After profiling is completed at level 3 the transition is made to level 4. Again, the length of the C2 queue is used as a feedback to adjust the thresholds.
After the first C1 compile some basic information is determined about the code like the number of the blocks and the number of the loops. Based on that it can be decided that a method is trivial and compiling it with C1 will yield the same code. In this case the method is compiled at level 1 instead of 4.

=== blocks?

back to school

one form of representation of program is CFG (control flow graph)

=== !

[quote,,Wikipedia]
    In a control-flow graph each node in the graph represents a basic block, i.e. a straight-line piece of code without any jumps or jump targets; jump targets start a block, and jumps end a block. Directed edges are used to represent jumps in the control flow. There are, in most presentations, two specially designated blocks: the entry block, through which control enters into the flow graph, and the exit block, through which all control flow leaves

=== !

Command line options: +
- `Tier?InvokeNotifyFreqLog` and `Tier?BackedgeNotifyFreqLog` control the frequency of method invocation and backedge notifications. Basically every n-th invocation or backedge a mutator thread makes a call into the runtime.

- `Tier?InvocationThreshold`, `Tier?CompileThreshold`, `Tier?BackEdgeThreshold`, `Tier?MinInvocationThreshold` control compilation thresholds.
Level 2 thresholds are not used and are provided for option-compatibility and potential future use.

=== !

Other thresholds work as follows:
Transition from interpreter (level 0) to C1 with full profiling (level 3) happens when the following predicate is true (X is the level):

   i > TierXInvocationThreshold * s || (i > TierXMinInvocationThreshold * s  && i + b > TierXCompileThreshold * s),

where $i$ is the number of method invocations, $b$ number of backedges and $s$ is the scaling coefficient that will be discussed further.

=== !

The intuition is to equalize the time that is spend profiling each method.
The same predicate is used to control the transition from level 3 to level 4 (C2). It should be noted though that the thresholds are relative. Moreover i and b for the 0->3 transition come from Method* and for 3->4 transition they come from MDO (since profiled invocations are counted separately). Finally, if a method does not contain anything worth profiling, a transition from level 3 to level 4 occurs without considering thresholds (e.g., with fewer invocations than what is specified by Tier4InvocationThreshold).

===  Method* and MDO

`Method` is a JVM class which models bytecode method

`src/hotspot/share/oops/method.hpp`

=== !

[source,cpp]
----
class Method : public Metadata {
 friend class VMStructs;
 friend class JVMCIVMStructs;
 private:
  // If you add a new field that points to any metaspace object, you
  // must add this field to Method::metaspace_pointers_do().
  ConstMethod*      _constMethod;                // Method read-only data.
  MethodData*       _method_data;
  MethodCounters*   _method_counters;
}
----

=== !

MDO are instances of `MethodData`, +
which are only use at compilation level 3, +
(there are traces that it can be also used by interpreter)
so they are not always available +
(your intuition is right C2 methods don't have profilers)

=== !

[source,cpp]
----
class MethodCounters : public Metadata {
 friend class VMStructs;
 friend class JVMCIVMStructs;
 private:
  InvocationCounter _invocation_counter;         // Incremented before each activation of the method - used to trigger frequency-based optimizations
  InvocationCounter _backedge_counter;           // Incremented before each backedge taken - used to trigger frequency-based optimizations
  jlong             _prev_time;                   // Previous time the rate was acquired
  float             _rate;                        // Events (invocation and backedge counter increments) per millisecond
  int               _nmethod_age;
  int               _invoke_mask;                 // per-method Tier0InvokeNotifyFreqLog
  int               _backedge_mask;               // per-method Tier0BackedgeNotifyFreqLog
  int               _prev_event_count;            // Total number of events saved at previous callback
#if COMPILER2_OR_JVMCI
  u2                _interpreter_throwout_count; // Count of times method was exited via exception while interpreting
#endif
#if INCLUDE_JVMTI
  u2                _number_of_breakpoints;      // fullspeed debugging support
#endif
  // NMethod age is a counter for warm methods detection in the code cache sweeper.
  // The counter is reset by the sweeper and is decremented by some of the compiled
  // code. The counter values are interpreted as follows:
  // 1. (HotMethodDetection..INT_MAX] - initial value, no counters inserted
  // 2. [1..HotMethodDetectionLimit)  - the method is warm, the counter is used
  //                                    to figure out which methods can be flushed.
  // 3. (INT_MIN..0]                  - method is hot and will deopt and get
  //                                    recompiled without the counters
  u1                _highest_comp_level;          // Highest compile level this method has ever seen.
  u1                _highest_osr_comp_level;      // Same for OSR level
----

=== MethodData

as you saw `MethodCounter` has a really basic +
profiling information (only counters), +
the magic is in `MethodData`

`src/hotspot/share/oops/methodData.hpp`

=== !

The MethodData object collects counts and other profile information
during zeroth-tier (interpretive) and first-tier execution.
The profile is used later by compilation heuristics.  Some heuristics
enable use of aggressive (or "heroic") optimizations.  An aggressive
optimization often has a down-side, a corner case that it handles
poorly, but which is thought to be rare.  The profile provides
evidence of this rarity for a given method or even BCI.  It allows
the compiler to back out of the optimization at places where it
has historically been a poor choice.  Other heuristics try to use
specific information gathered about types observed at a given site.

=== !

All data in the profile is approximate. It is expected to be accurate
on the whole, but the system expects occasional inaccuraces, due to
counter overflow, multiprocessor races during data collection, space
limitations, missing MDO blocks, etc.  Bad or missing data will degrade
optimization quality but will not affect correctness.  Also, each MDO
is marked with its birth-date ("creation_mileage") which can be used
to assess the quality ("maturity") of its data.

=== !

Short (<32-bit) counters are designed to overflow to a known "saturated"
state.  Also, certain recorded per-BCI events are given one-bit counters
which overflow to a saturated state which applied to all counters at
that BCI.  In other words, there is a small lattice which approximates
the ideal of an infinite-precision counter for each event at each BCI,
and the lattice quickly "bottoms out" in a state where all counters
are taken to be indefinitely large.

The reader will find many data races in profile gathering code, starting
with invocation counter incrementation. None of these races harm correct
execution of the compiled code.

=== speculating JIT

image::https://media.giphy.com/media/Lw39ENuDr0SdO/giphy.gif[background]

=== !

[source,Java]
----
if(){
  // lots of code
} else {
  // even more code
}
----

=== !

[source,Java]
----
if(){
  // lots of code
} else {
  uncommon_trap();
}
----

=== uncommon trap

this is a place in code of your compiled method +
which calls into JVM `Deoptimization::uncommon_trap`

`src/hotspot/share/runtime/deoptimization.hpp`

=== deopt reason & action

DepotReason is a condition that cause deoptimization

* unexpected null or zero divisor
* unexpected array index
* unexpected object class
* unexpected object class in bimorphic inlining

=== deopt action

[source,cpp]
----
  // What action must be taken by the runtime?
  enum DeoptAction {
    Action_none,                  // just interpret, do not invalidate nmethod
    Action_maybe_recompile,       // recompile the nmethod; need not invalidate
    Action_reinterpret,           // invalidate the nmethod, reset IC, maybe recompile
    Action_make_not_entrant,      // invalidate the nmethod, recompile (probably)
    Action_make_not_compilable,   // invalidate the nmethod and do not compile
    Action_LIMIT
    // Note:  Keep this enum in sync. with _trap_action_name.
  };
----

=== so what is actually happening?

* your compile frame needs to be converted into an interpreter frame
* compiled method can be marked as not entrant, or even invalidated from code cache
* invocation counters and method data can be reset

